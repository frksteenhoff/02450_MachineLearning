{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Artificial Neural Networks and Bias/Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import neurolab as nl\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import cross_validation\n",
    "#from toolbox_02450 import rlr_validate, dbfplot\n",
    "from matplotlib.pyplot import (figure, plot, subplot, title, xlabel, ylabel, \n",
    "                               hold, contour, contourf, cm, colorbar, show,\n",
    "                               legend, semilogx, loglog)\n",
    "\n",
    "from pybrain.datasets            import ClassificationDataSet\n",
    "from pybrain.tools.shortcuts     import buildNetwork\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.structure.modules   import SoftmaxLayer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 8.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-e7d16d90ae9e>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-e7d16d90ae9e>\"\u001b[1;36m, line \u001b[1;32m44\u001b[0m\n\u001b[1;33m    Xty = X_train.T @ y_train\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# exercise 8.1.1\n",
    "mat_data = loadmat('Data/body.mat')\n",
    "X = mat_data['X']\n",
    "y = mat_data['y']#.squeeze()\n",
    "attributeNames = [name[0] for name in mat_data['attributeNames'][0]]\n",
    "N, M = X.shape\n",
    "\n",
    "# Add offset attribute\n",
    "X = np.concatenate((np.ones((X.shape[0],1)),X),1)\n",
    "attributeNames = [u'Offset']+attributeNames\n",
    "M = M+1\n",
    "\n",
    "## Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 5\n",
    "CV = cross_validation.KFold(N,K)\n",
    "\n",
    "# Values of lambda\n",
    "lambdas = np.power(10.,range(-5,9))\n",
    "\n",
    "# Initialize variables\n",
    "#T = len(lambdas)\n",
    "Error_train = np.empty((K,1))\n",
    "Error_test = np.empty((K,1))\n",
    "Error_train_rlr = np.empty((K,1))\n",
    "Error_test_rlr = np.empty((K,1))\n",
    "Error_train_nofeatures = np.empty((K,1))\n",
    "Error_test_nofeatures = np.empty((K,1))\n",
    "w_rlr = np.matrix(np.empty((M,K)))\n",
    "w_noreg = np.matrix(np.empty((M,K)))\n",
    "\n",
    "k=0\n",
    "for train_index, test_index in CV:\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    internal_cross_validation = 10    \n",
    "    \n",
    "    opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda = rlr_validate(X_train, y_train, lambdas, internal_cross_validation)\n",
    "\n",
    "    Xty = X_train.T @ y_train\n",
    "    XtX = X_train.T @ X_train\n",
    "    \n",
    "    # Compute mean squared error without using the input data at all\n",
    "    Error_train_nofeatures[k] = np.square(y_train-y_train.mean()).sum()/y_train.shape[0]\n",
    "    Error_test_nofeatures[k] = np.square(y_test-y_test.mean()).sum()/y_test.shape[0]\n",
    "\n",
    "    # Estimate weights for the optimal value of lambda, on entire training set\n",
    "    w_rlr[:,k] = np.linalg.lstsq(XtX+opt_lambda*np.eye(M),Xty)[0]\n",
    "    # Compute mean squared error with regularization with optimal lambda\n",
    "    Error_train_rlr[k] = np.square(y_train-X_train @ w_rlr[:,k]).sum()/y_train.shape[0]\n",
    "    Error_test_rlr[k] = np.square(y_test-X_test @ w_rlr[:,k]).sum()/y_test.shape[0]\n",
    "\n",
    "    # Estimate weights for unregularized linear regression, on entire training set\n",
    "    w_noreg[:,k] = np.linalg.lstsq(XtX,Xty)[0]\n",
    "    # Compute mean squared error without regularization\n",
    "    Error_train[k] = np.square(y_train-X_train @ w_noreg[:,k]).sum()/y_train.shape[0]\n",
    "    Error_test[k] = np.square(y_test-X_test @ w_noreg[:,k]).sum()/y_test.shape[0]\n",
    "    # OR ALTERNATIVELY: you can use sklearn.linear_model module for linear regression:\n",
    "    #m = lm.LinearRegression().fit(X_train, y_train)\n",
    "    #Error_train[k] = np.square(y_train-m.predict(X_train)).sum()/y_train.shape[0]\n",
    "    #Error_test[k] = np.square(y_test-m.predict(X_test)).sum()/y_test.shape[0]\n",
    "\n",
    "    figure(k, figsize=(12,8))\n",
    "    subplot(1,2,1)\n",
    "    semilogx(lambdas,mean_w_vs_lambda.T,'.-')\n",
    "    xlabel('Regularization factor')\n",
    "    ylabel('Mean Coefficient Values')    \n",
    "    \n",
    "    subplot(1,2,2)\n",
    "    title('Optimal lambda = {0}'.format(opt_lambda))\n",
    "    loglog(lambdas,train_err_vs_lambda.T,'b.-',lambdas,test_err_vs_lambda.T,'r.-')\n",
    "    xlabel('Regularization factor')\n",
    "    ylabel('Squared error (crossvalidation)')\n",
    "    legend(['Train error','Validation error'])\n",
    "    \n",
    "    print('Cross validation fold {0}/{1}:'.format(k+1,K))\n",
    "    print('Train indices: {0}'.format(train_index))\n",
    "    print('Test indices: {0}\\n'.format(test_index))\n",
    "\n",
    "    k+=1\n",
    "\n",
    "# Display results\n",
    "print('\\n')\n",
    "print('Linear regression without feature selection:\\n')\n",
    "print('- Training error: {0}'.format(Error_train.mean()))\n",
    "print('- Test error:     {0}'.format(Error_test.mean()))\n",
    "print('- R^2 train:     {0}'.format((Error_train_nofeatures.sum()-Error_train.sum())/Error_train_nofeatures.sum()))\n",
    "print('- R^2 test:     {0}\\n'.format((Error_test_nofeatures.sum()-Error_test.sum())/Error_test_nofeatures.sum()))\n",
    "print('Regularized Linear regression:')\n",
    "print('- Training error: {0}'.format(Error_train_rlr.mean()))\n",
    "print('- Test error:     {0}'.format(Error_test_rlr.mean()))\n",
    "print('- R^2 train:     {0}'.format((Error_train_nofeatures.sum()-Error_train_rlr.sum())/Error_train_nofeatures.sum()))\n",
    "print('- R^2 test:     {0}\\n'.format((Error_test_nofeatures.sum()-Error_test_rlr.sum())/Error_test_nofeatures.sum()))\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exercise 8.2.2\n",
    "# read XOR DATA from matlab datafile\n",
    "mat_data = loadmat('../Data/xor.mat')\n",
    "X = mat_data['X']\n",
    "y = mat_data['y']\n",
    "\n",
    "\n",
    "attributeNames = [name[0] for name in mat_data['attributeNames'].squeeze()]\n",
    "classNames = [name[0] for name in mat_data['classNames'].squeeze()]\n",
    "N, M = X.shape\n",
    "C = len(classNames)\n",
    "\n",
    "\n",
    "# Parameters for neural network classifier\n",
    "n_hidden_units = 1      # number of hidden units\n",
    "n_train = 2             # number of networks trained in each k-fold\n",
    "\n",
    "# These parameters are usually adjusted to: (1) data specifics, (2) computational constraints\n",
    "learning_goal = 2.0     # stop criterion 1 (train mse to be reached)\n",
    "max_epochs = 200        # stop criterion 2 (max epochs in training)\n",
    "\n",
    "# K-fold CrossValidation (4 folds here to speed up this example)\n",
    "K = 4\n",
    "CV = cross_validation.KFold(N,K,shuffle=True)\n",
    "\n",
    "# Variable for classification error\n",
    "errors = np.zeros(K)\n",
    "error_hist = np.zeros((max_epochs,K))\n",
    "bestnet = list()\n",
    "k=0\n",
    "for train_index, test_index in CV:\n",
    "    print('\\nCrossvalidation fold: {0}/{1}'.format(k+1,K))    \n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index,:]\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index,:]\n",
    "    \n",
    "    best_train_error = 1e100\n",
    "    for i in range(n_train):\n",
    "        # Create randomly initialized network with 2 layers\n",
    "        ann = nl.net.newff([[0, 1], [0, 1]], [n_hidden_units, 1], [nl.trans.TanSig(),nl.trans.PureLin()])\n",
    "        # train network\n",
    "        train_error = ann.train(X_train, y_train, goal=learning_goal, epochs=max_epochs, show=round(max_epochs/8))\n",
    "        if train_error[-1]<best_train_error:\n",
    "            bestnet.append(ann)\n",
    "            best_train_error = train_error[-1]\n",
    "            error_hist[range(len(train_error)),k] = train_error\n",
    "    \n",
    "    y_est = bestnet[k].sim(X_test)\n",
    "    y_est = (y_est>.5).astype(int)\n",
    "    errors[k] = (y_est!=y_test).sum().astype(float)/y_test.shape[0]\n",
    "    k+=1\n",
    "    \n",
    "\n",
    "# Print the average classification error rate\n",
    "print('Error rate: {0}%'.format(100*np.mean(errors)))\n",
    "\n",
    "\n",
    "# Display the decision boundary for the several crossvalidation folds.\n",
    "# (create grid of points, compute network output for each point, color-code and plot).\n",
    "grid_range = [-1, 2, -1, 2]; delta = 0.05; levels = 100\n",
    "a = np.arange(grid_range[0],grid_range[1],delta)\n",
    "b = np.arange(grid_range[2],grid_range[3],delta)\n",
    "A, B = np.meshgrid(a, b)\n",
    "values = np.zeros(A.shape)\n",
    "\n",
    "figure(1,figsize=(18,9)); hold(True)\n",
    "for k in range(4):\n",
    "    subplot(2,2,k+1)\n",
    "    cmask = (y==0).squeeze(); plot(X[cmask,0], X[cmask,1],'.r')\n",
    "    cmask = (y==1).squeeze(); plot(X[cmask,0], X[cmask,1],'.b')\n",
    "    title('Model prediction and decision boundary (kfold={0})'.format(k+1))\n",
    "    xlabel('Feature 1'); ylabel('Feature 2');\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            values[i,j] = bestnet[k].sim( np.mat([a[i],b[j]]) )[0,0]\n",
    "    contour(A, B, values, levels=[.5], colors=['k'], linestyles='dashed')\n",
    "    contourf(A, B, values, levels=np.linspace(values.min(),values.max(),levels), cmap=cm.RdBu)\n",
    "    if k==0: colorbar(); legend(['Class A (y=0)', 'Class B (y=1)'])\n",
    "\n",
    "\n",
    "# Display exemplary networks learning curve (best network of each fold)\n",
    "figure(2); hold(True)\n",
    "bn_id = np.argmax(error_hist[-1,:])\n",
    "error_hist[error_hist==0] = learning_goal\n",
    "for bn_id in range(K):\n",
    "    plot(error_hist[:,bn_id]); xlabel('epoch'); ylabel('train error (mse)'); title('Learning curve (best for each CV fold)')\n",
    "\n",
    "plot(range(max_epochs), [learning_goal]*max_epochs, '-.')\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exercise 8.2.5\n",
    "# Load Matlab data file and extract variables of interest\n",
    "mat_data = loadmat('..\\\\Data\\\\wine2.mat')\n",
    "attributeNames = [name[0] for name in mat_data['attributeNames'][0]]\n",
    "X = mat_data['X']\n",
    "y = mat_data['y']\n",
    "#Downsample: X = X[1:20,:] y = y[1:20,:]\n",
    "\n",
    "N, M = X.shape\n",
    "C = 2\n",
    "# Normalize data\n",
    "X = stats.zscore(X);\n",
    "\n",
    "# Parameters for neural network classifier\n",
    "n_hidden_units = 2     # number of hidden units\n",
    "n_train = 2             # number of networks trained in each k-fold\n",
    "learning_goal = 10      # stop criterion 1 (train mse to be reached)\n",
    "max_epochs = 64         # stop criterion 2 (max epochs in training)\n",
    "show_error_freq = 3     # frequency of training status updates\n",
    "\n",
    "\n",
    "# K-fold crossvalidation\n",
    "K = 3                   # only five folds to speed up this example\n",
    "CV = cross_validation.KFold(N,K,shuffle=True)\n",
    "\n",
    "# Variable for classification error\n",
    "errors = np.zeros(K)\n",
    "error_hist = np.zeros((max_epochs,K))\n",
    "bestnet = list()\n",
    "k=0\n",
    "for train_index, test_index in CV:\n",
    "    print('\\nCrossvalidation fold: {0}/{1}'.format(k+1,K))    \n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index,:]\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index,:]\n",
    "    \n",
    "    best_train_error = 1e100\n",
    "    for i in range(n_train):\n",
    "        print('Training network {0}/{1}...'.format(i+1,n_train))\n",
    "        # Create randomly initialized network with 2 layers\n",
    "        ann = nl.net.newff([[-3, 3]]*M, [n_hidden_units, 1], [nl.trans.TanSig(),nl.trans.PureLin()])\n",
    "        if i==0:\n",
    "            bestnet.append(ann)\n",
    "        # train network\n",
    "        train_error = ann.train(X_train, y_train, goal=learning_goal, epochs=max_epochs, show=show_error_freq)\n",
    "        if train_error[-1]<best_train_error:\n",
    "            bestnet[k]=ann\n",
    "            best_train_error = train_error[-1]\n",
    "            error_hist[range(len(train_error)),k] = train_error\n",
    "\n",
    "    print('Best train error: {0}...'.format(best_train_error))\n",
    "    y_est = bestnet[k].sim(X_test)\n",
    "    y_est = (y_est>.5).astype(int)\n",
    "    errors[k] = (y_est!=y_test).sum().astype(float)/y_test.shape[0]\n",
    "    k+=1\n",
    "    \n",
    "\n",
    "# Print the average classification error rate\n",
    "print('Error rate: {0}%'.format(100*np.mean(errors)))\n",
    "\n",
    "\n",
    "figure();\n",
    "subplot(2,1,1); bar(range(0,K),errors); title('CV errors');\n",
    "subplot(2,1,2); plot(error_hist); title('Training error as function of BP iterations');\n",
    "figure();\n",
    "subplot(2,1,1); plot(y_est); plot(y_test); title('Last CV-fold: est_y vs. test_y'); \n",
    "subplot(2,1,2); plot((y_est-y_test)); title('Last CV-fold: prediction error (est_y-test_y)'); \n",
    "\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exercise 8.2.6\n",
    "# Load Matlab data file and extract variables of interest\n",
    "mat_data = loadmat('..\\\\Data\\\\wine2.mat')\n",
    "attributeNames = [name[0] for name in mat_data['attributeNames'][0]]\n",
    "X = mat_data['X']\n",
    "y = X[:,10]             # alcohol contents (target)\n",
    "X = X[:,1:10]           # the rest of features\n",
    "N, M = X.shape\n",
    "C = 2\n",
    "\n",
    "# Normalize data\n",
    "X = stats.zscore(X);\n",
    "\n",
    "# Normalize and compute PCA (UNCOMMENT to experiment with PCA preprocessing)\n",
    "#Y = stats.zscore(X,0);\n",
    "#U,S,V = np.linalg.svd(Y,full_matrices=False)\n",
    "#V = V.T\n",
    "# Components to be included as features\n",
    "#k_pca = 3\n",
    "#X = X @ V[:,0:k_pca]\n",
    "#N, M = X.shape\n",
    "\n",
    "\n",
    "# Parameters for neural network classifier\n",
    "n_hidden_units = 2      # number of hidden units\n",
    "n_train = 2             # number of networks trained in each k-fold\n",
    "learning_goal = 100     # stop criterion 1 (train mse to be reached)\n",
    "max_epochs = 64         # stop criterion 2 (max epochs in training)\n",
    "show_error_freq = 5     # frequency of training status updates\n",
    "\n",
    "# K-fold crossvalidation\n",
    "K = 3                   # only five folds to speed up this example\n",
    "CV = cross_validation.KFold(N,K,shuffle=True)\n",
    "\n",
    "# Variable for classification error\n",
    "errors = np.zeros(K)\n",
    "error_hist = np.zeros((max_epochs,K))\n",
    "bestnet = list()\n",
    "k=0\n",
    "for train_index, test_index in CV:\n",
    "    print('\\nCrossvalidation fold: {0}/{1}'.format(k+1,K))    \n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index,:]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index,:]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    best_train_error = 1e100\n",
    "    for i in range(n_train):\n",
    "        print('Training network {0}/{1}...'.format(i+1,n_train))\n",
    "        # Create randomly initialized network with 2 layers\n",
    "        ann = nl.net.newff([[-3, 3]]*M, [n_hidden_units, 1], [nl.trans.TanSig(),nl.trans.PureLin()])\n",
    "        if i==0:\n",
    "            bestnet.append(ann)\n",
    "        # train network\n",
    "        train_error = ann.train(X_train, y_train.reshape(-1,1), goal=learning_goal, epochs=max_epochs, show=show_error_freq)\n",
    "        if train_error[-1]<best_train_error:\n",
    "            bestnet[k]=ann\n",
    "            best_train_error = train_error[-1]\n",
    "            error_hist[range(len(train_error)),k] = train_error\n",
    "\n",
    "    print('Best train error: {0}...'.format(best_train_error))\n",
    "    y_est = bestnet[k].sim(X_test).squeeze()\n",
    "    errors[k] = np.power(y_est-y_test,2).sum().astype(float)/y_test.shape[0]\n",
    "    k+=1\n",
    "    \n",
    "\n",
    "# Print the average least squares error\n",
    "print('Mean-square error: {0}'.format(np.mean(errors)))\n",
    "\n",
    "figure();\n",
    "subplot(2,1,1); bar(range(0,K),errors); title('Mean-square errors');\n",
    "subplot(2,1,2); plot(error_hist); title('Training error as function of BP iterations');\n",
    "figure();\n",
    "subplot(2,1,1); plot(y_est); plot(y_test); title('Last CV-fold: est_y vs. test_y'); \n",
    "subplot(2,1,2); plot((y_est-y_test)); title('Last CV-fold: prediction error (est_y-test_y)'); \n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exercise 8.3.1 Fit neural network classifiers using softmax output weighting\n",
    "\n",
    "# Load Matlab data file and extract variables of interest\n",
    "mat_data = loadmat('Data/synth1.mat')\n",
    "X = np.matrix(mat_data['X'])\n",
    "X = X - np.ones((X.shape[0],1)) * mean(X,0)\n",
    "X_train = np.matrix(mat_data['X_train'])\n",
    "X_test = np.matrix(mat_data['X_test'])\n",
    "y = np.matrix(mat_data['y']) \n",
    "y_train = np.matrix(mat_data['y_train'])\n",
    "y_test = np.matrix(mat_data['y_test'])\n",
    "#attributeNames = [name[0] for name in mat_data['attributeNames'].squeeze()]\n",
    "classNames = [name[0][0] for name in mat_data['classNames']]\n",
    "N, M = X.shape\n",
    "C = len(classNames)\n",
    "NHiddenUnits = 2;\n",
    "\n",
    "#%% convert to ClassificationDataSet format. \n",
    "def conv2DS(Xv,yv = None) :\n",
    "    if yv == None : \n",
    "        yv =  np.asmatrix( np.ones( (Xv.shape[0],1) ) )\n",
    "        for j in range(len(classNames)) : yv[j] = j\n",
    "            \n",
    "    C = len(unique(yv.flatten().tolist()[0]))\n",
    "    DS = ClassificationDataSet(M, 1, nb_classes=C)\n",
    "    for i in range(Xv.shape[0]) : DS.appendLinked(Xv[i,:].tolist()[0], [yv[i].A[0][0]])\n",
    "    DS._convertToOneOfMany( )\n",
    "    return DS    \n",
    "\n",
    "DS_train = conv2DS(X_train,y_train)\n",
    "DS_test = conv2DS(X_test,y_test)\n",
    "\n",
    "fnn = buildNetwork(  DS_train.indim, NHiddenUnits, DS_train.outdim, outclass=SoftmaxLayer,bias=True )    \n",
    "trainer = BackpropTrainer( fnn, dataset=DS_train, momentum=0.1, verbose=True, weightdecay=0.01)\n",
    "# Train for 100 iterations. \n",
    "for i in range(50): trainer.trainEpochs( 1 )\n",
    "ote = fnn.activateOnDataset(DS_test)\n",
    "\n",
    "ErrorRate = (np.argmax(ote,1) != y_test.T).mean(dtype=float)\n",
    "print('Error rate (ensemble): {0}%'.format(100*ErrorRate))\n",
    "figure(1)\n",
    "def neval(xval):\n",
    "    return argmax(fnn.activateOnDataset(conv2DS(np.asmatrix(xval)) ),1) \n",
    "\n",
    "dbplotf(X_test,y_test,neval,'auto')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exercise 8.3.2 Fit neural network classifiers using softmax output weighting\n",
    "# Load Matlab data file and extract variables of interest\n",
    "mat_data = loadmat('Data/synth1.mat')\n",
    "X = np.matrix(mat_data['X'])\n",
    "X = X - np.ones((X.shape[0],1)) * mean(X,0)\n",
    "X_train = np.matrix(mat_data['X_train'])\n",
    "X_test = np.matrix(mat_data['X_test'])\n",
    "y = np.matrix(mat_data['y']) \n",
    "y_train = np.matrix(mat_data['y_train'])\n",
    "y_test = np.matrix(mat_data['y_test'])\n",
    "#attributeNames = [name[0] for name in mat_data['attributeNames'].squeeze()]\n",
    "classNames = [name[0][0] for name in mat_data['classNames']]\n",
    "N, M = X.shape\n",
    "C = len(classNames)\n",
    "\n",
    "#%% convert to ClassificationDataSet format. \n",
    "def conv2DS(Xv,yv = None) :\n",
    "    if yv == None : \n",
    "        yv =  np.asmatrix( np.ones( (Xv.shape[0],1) ) )\n",
    "        for j in range(len(classNames)) : yv[j] = j\n",
    "            \n",
    "    C = len(unique(yv.flatten().tolist()[0]))\n",
    "    DS = ClassificationDataSet(M, 1, nb_classes=C)\n",
    "    for i in range(Xv.shape[0]) : DS.appendLinked(Xv[i,:].tolist()[0], [yv[i].A[0][0]])\n",
    "    DS._convertToOneOfMany( )\n",
    "    return DS    \n",
    "\n",
    "DS_train = conv2DS(X_train,y_train)\n",
    "DS_test = conv2DS(X_test,y_test)\n",
    "\n",
    "# A neural network without a hidden layer will simulate logistic regression (albeit very slowly)\n",
    "fnn = buildNetwork(  DS_train.indim, DS_train.outdim, outclass=SoftmaxLayer,bias=True )    \n",
    "trainer = BackpropTrainer( fnn, dataset=DS_train, momentum=0.1, verbose=True, weightdecay=0.01)\n",
    "# Train for 100 iterations. \n",
    "for i in range(50): trainer.trainEpochs( 1 )\n",
    "ote = fnn.activateOnDataset(DS_test)\n",
    "\n",
    "ErrorRate = (np.argmax(ote,1) != y_test.T).mean(dtype=float)\n",
    "print('Error rate (ensemble): {0}%'.format(100*ErrorRate))\n",
    "figure(1)\n",
    "def neval(xval):\n",
    "    return argmax(fnn.activateOnDataset(conv2DS(np.asmatrix(xval)) ),1) \n",
    "\n",
    "dbplotf(X_test,y_test,neval,'auto')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
