{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 - AUC and Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "from scipy.io import loadmat\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from toolbox_02450 import rocplot, confmatplot\n",
    "#rom toolbox_02450 import dbplot, dbprobplot, bootstrap\n",
    "from bin_classifier_ensemble import BinClassifierEnsemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from toolbox_02450 import rocplot, confmatplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 9.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exercise 9.1.1\n",
    "# Load Matlab data file and extract variables of interest\n",
    "mat_data = loadmat('Data/wine2.mat')\n",
    "X = np.matrix(mat_data['X'])\n",
    "y = np.matrix(mat_data['y'], dtype=int)\n",
    "attributeNames = [name[0] for name in mat_data['attributeNames'][0]]\n",
    "classNames = [name[0][0] for name in mat_data['classNames']]\n",
    "N, M = X.shape\n",
    "C = len(classNames)\n",
    "\n",
    "# K-fold crossvalidation\n",
    "K = 2\n",
    "CV = cross_validation.StratifiedKFold(y.A.ravel().tolist(),K)\n",
    "\n",
    "k=0\n",
    "for train_index, test_index in CV:\n",
    "\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train, y_train = X[train_index,:], y[train_index,:]\n",
    "    X_test, y_test = X[test_index,:], y[test_index,:]\n",
    "\n",
    "    logit_classifier = LogisticRegression()\n",
    "    logit_classifier.fit(X_train, y_train.A.ravel())\n",
    "\n",
    "    y_test_est = np.mat(logit_classifier.predict(X_test)).T\n",
    "    p = np.mat(logit_classifier.predict_proba(X_test)[:,1]).T\n",
    "\n",
    "    figure(k)\n",
    "    rocplot(p, y_test)\n",
    "\n",
    "    figure(k+1)\n",
    "    confmatplot(y_test,y_test_est)\n",
    "\n",
    "    k+=2\n",
    "    \n",
    "show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exercise 9.1.2\n",
    "# Load Matlab data file and extract variables of interest\n",
    "mat_data = loadmat('Data/wine2.mat')\n",
    "X = np.matrix(mat_data['X'])\n",
    "y = np.matrix(mat_data['y'], dtype=int)\n",
    "attributeNames = [name[0] for name in mat_data['attributeNames'][0]]\n",
    "classNames = [name[0][0] for name in mat_data['classNames']]\n",
    "\n",
    "attribute_included = 10   # alcohol contents\n",
    "X = X[:,attribute_included]\n",
    "attributeNames = attributeNames[attribute_included]\n",
    "N, M = X.shape\n",
    "C = len(classNames)\n",
    "\n",
    "# K-fold crossvalidation\n",
    "K = 2\n",
    "CV = cross_validation.StratifiedKFold(y.A.ravel().tolist(),K)\n",
    "\n",
    "k=0\n",
    "for train_index, test_index in CV:\n",
    "\n",
    "    # extract training and test set for current CV fold\n",
    "    X_train, y_train = X[train_index,:], y[train_index,:]\n",
    "    X_test, y_test = X[test_index,:], y[test_index,:]\n",
    "\n",
    "    logit_classifier = LogisticRegression()\n",
    "    logit_classifier.fit(X_train, y_train.A.ravel())\n",
    "\n",
    "    y_test_est = np.mat(logit_classifier.predict(X_test)).T\n",
    "    p = np.mat(logit_classifier.predict_proba(X_test)[:,1]).T\n",
    "\n",
    "    figure(k)\n",
    "    rocplot(p,y_test)\n",
    "\n",
    "    figure(k+1)\n",
    "    confmatplot(y_test,y_test_est)\n",
    "\n",
    "    k+=2\n",
    "    \n",
    "show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 9.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exercise 9.2.1\n",
    "# Load Matlab data file and extract variables of interest\n",
    "mat_data = loadmat('Data/synth5.mat')\n",
    "X = np.matrix(mat_data['X'])\n",
    "y = np.matrix(mat_data['y'])\n",
    "attributeNames = [name[0] for name in mat_data['attributeNames'].squeeze()]\n",
    "classNames = [name[0][0] for name in mat_data['classNames']]\n",
    "N, M = X.shape\n",
    "C = len(classNames)\n",
    "\n",
    "\n",
    "# Fit model using bootstrap aggregation (bagging):\n",
    "\n",
    "# Number of rounds of bagging\n",
    "L = 100\n",
    "\n",
    "# Weights for selecting samples in each bootstrap\n",
    "weights = np.ones((N,1),dtype=float)/N\n",
    "\n",
    "# Storage of trained log.reg. classifiers fitted in each bootstrap\n",
    "logits = [0]*L\n",
    "votes = np.zeros((N,1))\n",
    "\n",
    "# For each round of bagging\n",
    "for l in range(L):\n",
    "\n",
    "    # Extract training set by random sampling with replacement from X and y\n",
    "    X_train, y_train = bootstrap(X, y, N, weights)\n",
    "    \n",
    "    # Fit logistic regression model to training data and save result\n",
    "    logit_classifier = LogisticRegression()\n",
    "    logit_classifier.fit(X_train, y_train.A.ravel())\n",
    "    logits[l] = logit_classifier\n",
    "    y_est = np.mat(logit_classifier.predict(X)).T\n",
    "    votes = votes + y_est\n",
    "\n",
    "    ErrorRate = (y!=y_est).sum(dtype=float)/N\n",
    "    print('Error rate: {0}%'.format(ErrorRate*100))    \n",
    "    \n",
    "# Estimated value of class labels (using 0.5 as threshold) by majority voting\n",
    "y_est_ensemble = votes>(L/2)\n",
    "\n",
    "# Compute error rate\n",
    "ErrorRate = (y!=y_est_ensemble).sum(dtype=float)/N\n",
    "print('Error rate: {:.1f}%'.format(ErrorRate*100))\n",
    "\n",
    "ce = BinClassifierEnsemble(logits)\n",
    "figure(1); dbprobplot(ce, X, y, 'auto', resolution=200)\n",
    "figure(2); dbplot(ce, X, y, 'auto', resolution=200)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exercise 9.2.2\n",
    "# Load Matlab data file and extract variables of interest\n",
    "mat_data = loadmat('Data/synth5.mat')\n",
    "X = np.matrix(mat_data['X'])\n",
    "y = np.matrix(mat_data['y'])\n",
    "attributeNames = [name[0] for name in mat_data['attributeNames'].squeeze()]\n",
    "classNames = [name[0][0] for name in mat_data['classNames']]\n",
    "N, M = X.shape\n",
    "C = len(classNames)\n",
    "\n",
    "\n",
    "# Fit model using bootstrap aggregation (boosting, AdaBoost):\n",
    "\n",
    "# Number of rounds of bagging\n",
    "L = 10000\n",
    "\n",
    "# Weights for selecting samples in each bootstrap\n",
    "weights = np.ones((N,1),dtype=float)/N\n",
    "\n",
    "# Storage of trained log.reg. classifiers fitted in each bootstrap\n",
    "logits = [0]*L\n",
    "alpha = np.ones( (L,1) )\n",
    "votes = np.zeros((N,1))\n",
    "epsi = 0\n",
    "y_all = np.zeros((N,L))\n",
    "y = y > 0.5\n",
    "# For each round of bagging\n",
    "for l in range(L):\n",
    "    \n",
    "    # Extract training set by random sampling with replacement from X and y\n",
    "    while True : \n",
    "        # not a thing of beauty, however log.reg. fails if presented with less than two classes. \n",
    "        X_train, y_train = bootstrap(X, y, N, weights) \n",
    "        if not (all(y_train==0) or all(y_train == 1)) : break      \n",
    "    \n",
    "    # Fit logistic regression model to training data and save result\n",
    "    # turn off regularization with C. \n",
    "    logit_classifier = LogisticRegression(C=1000)\n",
    "\n",
    "    logit_classifier.fit(X_train, y_train.A.ravel()  )\n",
    "    logits[l] = logit_classifier\n",
    "    y_est = np.mat(logit_classifier.predict(X)).T > 0.5\n",
    "    \n",
    "    y_all[:,l] = 1.0 * y_est.ravel()\n",
    "    v  = matrix(y_est != y,dtype=float)\n",
    "    ErrorRate = multiply(weights,v).sum()\n",
    "    epsi = ErrorRate\n",
    "    \n",
    "    alphai = 0.5 * log( (1-epsi)/epsi)\n",
    "    \n",
    "    weights[y_est == y] = weights[y_est == y] * exp( -alphai )\n",
    "    weights[y_est != y] = weights[y_est != y] * exp(  alphai )\n",
    "    \n",
    "    weights = weights / sum(weights)\n",
    "            \n",
    "    votes = votes + y_est\n",
    "    alpha[l] = alphai\n",
    "    print('Error rate: {0}%'.format(ErrorRate*100))    \n",
    "    \n",
    "# Estimated value of class labels (using 0.5 as threshold) by majority voting\n",
    "alpha = mat(alpha)/sum(alpha)\n",
    "y_est_ensemble = mat(y_all) * alpha > 0.5\n",
    "\n",
    "#y_est_ensemble = votes > (L/2)\n",
    "#y_est_ensemble = mat(y_all) * mat(alpha) - (1-mat(y_all)) * mat(alpha) > 0\n",
    "ErrorRateEnsemble = sum(y_est_ensemble != y)/N\n",
    "\n",
    "# Compute error rate\n",
    "#ErrorRate = (y!=y_est_ensemble).sum(dtype=float)/N\n",
    "print('Error rate for ensemble classifier: {:.1f}%'.format(ErrorRateEnsemble*100))\n",
    " \n",
    "ce = BinClassifierEnsemble(logits)\n",
    "#figure(1); dbprobplot(ce, X, y, 'auto', resolution=200)\n",
    "#figure(2); dbplot(ce, X, y, 'auto', resolution=200)\n",
    "#figure(3); plot(alpha.A)\n",
    "\n",
    "#%%\n",
    "#ax = plt.subplots()\n",
    "#ax.set_color_cycle(['red', 'black', 'yellow'])\n",
    "plt.figure(5)\n",
    "plt.hold(True)\n",
    "\n",
    "for i in range(2):\n",
    "    plt.plot(X[ (y_est_ensemble==i).A.ravel(),0],X[ (y_est_ensemble==i).A.ravel(),1],'br'[i] + 'o')\n",
    "#    plt.plot(X[ (y_est_ensemble==1).A.ravel(),0],X[ (y_est_ensemble==1).A.ravel(),1],'x')\n",
    "    #plt.plot(x, i * x + i, label='$y = {i}x + {i}$'.format(i=i))\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exercise 9.2.3\n",
    "# Load Matlab data file and extract variables of interest\n",
    "mat_data = loadmat('Data/synth7.mat')\n",
    "X = np.matrix(mat_data['X'])\n",
    "y = np.matrix(mat_data['y'])\n",
    "attributeNames = [name[0] for name in mat_data['attributeNames'].squeeze()]\n",
    "classNames = [name[0][0] for name in mat_data['classNames']]\n",
    "N, M = X.shape\n",
    "C = len(classNames)\n",
    "\n",
    "\n",
    "# Number of rounds of bagging\n",
    "L = 100\n",
    "\n",
    "# Fit model using random tree classifier:\n",
    "rf_classifier = RandomForestClassifier(L)\n",
    "rf_classifier.fit(X.A, y.A.ravel())\n",
    "y_est = rf_classifier.predict(X).T\n",
    "y_est_prob = rf_classifier.predict_proba(X).T\n",
    "\n",
    "# Compute classification error\n",
    "ErrorRate = (y!=np.mat(y_est).T).sum(dtype=float)/N\n",
    "print('Error rate: {:.2f}%'.format(ErrorRate*100))    \n",
    "\n",
    "# Plot decision boundaries    \n",
    "figure(1); dbprobplot(rf_classifier, X, y, 'auto', resolution=400)\n",
    "figure(2); dbplot(rf_classifier, X, y, 'auto', resolution=400)\n",
    "\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
